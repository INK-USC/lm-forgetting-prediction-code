tulu_tasks:
  - open_orca
  - oasst1
  - lima
  - code_alpaca
  - gpt4_alpaca
  - cot
  - science
  - flan_v2
  - sharegpt
  - hard_coded
  - wizardlm

peft: "lora"
model_name: "allenai/OLMo-7B"
per_device_eval_batch_size: 1
per_device_train_batch_size: 1
save_mem: true
max_input_length: 1024
stream:
  bs: 1
  n_step_per_batch: 1

output_dir: "runs_olmo_ocl/tulu-train-5k/task_{TASK_ID}"

templates:
  task_id: "{TASK_ID}"

ocl:
  task_category: "tulu_train"
  task_id: "{TASK_ID}"

exp_group: "tulu"

is_lm_sft: true
ocl_val_max_batch: 100

grad_accum: 16
ocl_steps: 5000
max_epoch: 1000
ocl_val_step: 1000

gradient_accumulation_steps: 8
ans_start_pattern: "<|assistant|>"
tokenizer_name: "allenai/OLMo-7B-Instruct"